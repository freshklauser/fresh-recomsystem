目录

[TOC]



## 一、Basic

### 1. 推荐引擎和搜索引擎

- 搜索引擎

  存在**马太效应**的问题，即会造成越流行的东西随着搜索过程的迭代会越流行，使得那些越不流行的东西石沉大海

- 推荐引擎

  **长尾理论**（人们只关注曝光率高的项目，而忽略曝光率低的项目），长尾理论阐述了推荐系统所发挥的价值

## 二、UserCF

### 1. UserCF适用场景

### 2. UserCF 存在的问题

- 冷启动问题：

  新用户没有相似度高的用户，难以根据用户相似度推荐物品

- 对于一个物品，如果所有最近邻的用户都没有买这个物品，则UserCF永远不可能将该物品推荐给目标用户。但实际上，这个物品是目标用户希望被推荐的物品。

### 3. UserCF 基础优化解决方案

- 相似度计算最好使用皮尔斯相似度 `np.corrcoef(x,y)`
- 考虑共同打分物品的数目，如乘上`min(n, N)/N` (`n:`共同打分数; `N:`指定阈值)
- 对打分进行归一化处理`---> [0, 1]`
- 设置一个相似度阈值

### 4. 为什么UserCF不流行？

- 稀疏问题：计算成本高
- 数百万用户计算，需要计算两两之间的相似度，计算成本高
- 用户是变化的，需要经常更新维护表，维护成本高

## 三、ItemCF

### 1. ItemCF优势

- 计算性能高，通常用户数量远大于物品数量，且物品可以打标签
- 可预计计算保留，物品并不善变，对应的物品相似度表维护成本低

### 2. 冷启动问题

**用户冷启动**：

- 引导用户把自己的一些属性表达出来
- 利用现有的开放数据平台
- 根据用户注册属性
- 推荐排行榜单

**物品冷启动**：

- 文本分析
- 主题模型
- 打标签
- 推荐排行榜单

### 3. UserCF 和 ItemCF 对比

### 4 适用对象

UserCF(社会化): 实时新闻、突然情况

ItemCF(个性化): 图书、电子商务、电影 。。。

## 四、隐语义模型 `LFM`

### 1. 隐语义模型：

- 从数据出发，进行个性化推荐

- 用户和物品之间有隐含联系

- <font color=tomato>隐含因子让计算机能理解就好</font>

- 将用户和物品通过中介隐含因子联系起来

  `（N, M）---> (F,N), (F,M)` 与`SVD`有些类似（svd使用奇异值分解，lfm使用矩阵分解，复杂度比svd更低）

<div align=center><img src='./img/lfm.png' width=100%></div>

### 2. 矩阵分解

<div align=center><img src='./img/lfm2.png' width=100%></div>

其中：

- `R_ui`：用户`u`对物品`i`的评分`rate`;  
- `P_u:` 用户`u`与隐含因子之间的关系`(P_uk 更合适)`
- `Q_i:` 物品`i`与隐含因子之间的关系 `(Q_ki 更合适)`

公式2是`L2`正则化的均方误差(`MSE`)损失函数`loss function` 

### 3. 模型求解 `(P_uk, Q_ik)`

<div align=center><img src='./img/lfm3.png' width=100%></div>

### 4. 负样本选择

- 每个用户，保证正负样本的均衡（数目相似）
- 选取那些热门的，且用户却没有行为的物品作为负样本
- 对于用户--物品 集 `K{(u, i)}`, 若 `(u,i)`是正样本，则有`r_ui = 1;` 负样本则`r_ui = 0`

### 5. 参数选择

- 隐特征个数 `F`, 通常取 `F=100`
- 学习率`alpha`， 别太大即可
- 正则化参数 `lambda`， 别太大
- 负样本/正样本比例 `ratio`

### 6. LFM与CF的对比

- 原理： `CF`基于统计， `LFM`基于建模
- 空间复杂度，`LFM`模型较小
- 实时推荐依旧难，目前离线计算多
- `LFM`模型解释性不明显，隐含因子计算机能理解即可，没指定具体物理意义（类似神经网络的可解释性弱）

## 五、评估标准

准确率、召回率、覆盖率、新颖性

<div align=left><img src='./img/evatlfm.png' width=100%></div>

<div align=left><img src='./img/evatlfm1.png' width=75%></div>

